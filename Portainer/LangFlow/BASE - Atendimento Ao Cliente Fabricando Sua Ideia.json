{"id":"fb443850-d1d3-497c-ac03-5cbd4e3a3f1b","data":{"nodes":[{"id":"OllamaModel-I64ie","type":"genericNode","position":{"x":1823.4390104447507,"y":341.7978767949796},"data":{"type":"OllamaModel","node":{"template":{"_type":"Component","base_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"base_url","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":false,"type":"str"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model Name\",\n            value=\"llama2\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Input\",\n        ),\n        BoolInput(\n            name=\"stream\",\n            display_name=\"Stream\",\n            info=STREAM_INFO_TEXT,\n        ),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"format","display_name":"Format","advanced":true,"dynamic":false,"info":"Specify the format of the output (e.g., json).","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"metadata":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"metadata","display_name":"Metadata","advanced":true,"dynamic":false,"info":"Metadata to add to the run trace.","title_case":false,"type":"dict"},"mirostat":{"trace_as_metadata":true,"options":["Disabled","Mirostat","Mirostat 2.0"],"required":false,"placeholder":"","show":true,"value":"Disabled","name":"mirostat","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","title_case":false,"type":"str"},"mirostat_eta":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"mirostat_eta","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate for Mirostat algorithm. (Default: 0.1)","title_case":false,"type":"float"},"mirostat_tau":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"mirostat_tau","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls the balance between coherence and diversity of the output. (Default: 5.0)","title_case":false,"type":"float"},"model":{"trace_as_metadata":true,"options":["splitpierre/bode-alpaca-pt-br:latest","codeqwen:latest","qwen2:latest","nomic-embed-text:latest","mxbai-embed-large:latest","all-minilm:latest"],"required":false,"placeholder":"","show":true,"value":"qwen2:latest","name":"model","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.ai/library for more models.","refresh_button":true,"title_case":false,"type":"str"},"num_ctx":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_ctx","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating tokens. (Default: 2048)","title_case":false,"type":"int"},"num_gpu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_gpu","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)","title_case":false,"type":"int"},"num_thread":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_thread","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation. (Default: detected for optimal performance)","title_case":false,"type":"int"},"repeat_last_n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"repeat_last_n","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)","title_case":false,"type":"int"},"repeat_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"repeat_penalty","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text. (Default: 1.1)","title_case":false,"type":"float"},"stop_tokens":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"stop_tokens","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"Comma-separated list of tokens to signal the model to stop generating text.","title_case":false,"type":"str"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":false,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool"},"system":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system","display_name":"System","advanced":true,"dynamic":false,"info":"System to use for generating text.","title_case":false,"type":"str"},"system_message":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str"},"tags":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"tags","display_name":"Tags","advanced":true,"dynamic":false,"info":"Comma-separated list of tags to add to the run trace.","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.2,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","title_case":false,"type":"float"},"template":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"template","display_name":"Template","advanced":true,"dynamic":false,"info":"Template to use for generating text.","title_case":false,"type":"str"},"tfs_z":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"tfs_z","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling value. (Default: 1)","title_case":false,"type":"float"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"timeout","display_name":"Timeout","advanced":true,"dynamic":false,"info":"Timeout for the request stream.","title_case":false,"type":"int"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_k","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K. (Default: 40)","title_case":false,"type":"int"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_p","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works together with top-k. (Default: 0.9)","title_case":false,"type":"float"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"verbose","display_name":"Verbose","advanced":false,"dynamic":false,"info":"Whether to print out response text.","title_case":false,"type":"bool"}},"description":"Generate text using Ollama Local LLMs.","icon":"Ollama","base_classes":["LanguageModel","Message"],"display_name":"Ollama","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"hidden":false},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["base_url","model","temperature","format","metadata","mirostat","mirostat_eta","mirostat_tau","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","tfs_z","timeout","top_k","top_p","verbose","tags","stop_tokens","system","template","input_value","stream","system_message"],"beta":false,"edited":false},"id":"OllamaModel-I64ie"},"selected":false,"width":384,"height":774,"dragging":false,"positionAbsolute":{"x":1823.4390104447507,"y":341.7978767949796}},{"id":"ChatOutput-4s3S0","type":"genericNode","position":{"x":2233.468683836904,"y":835.5935957647246},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-4s3S0"},"selected":false,"width":384,"height":308,"positionAbsolute":{"x":2233.468683836904,"y":835.5935957647246},"dragging":false},{"id":"Prompt-TmJY6","type":"genericNode","position":{"x":1389.9191275914766,"y":589.778611834799},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"{context}\n\n---\n\nGiven the context above, answer the question as best as possible.\n\nIf the user shows intention to purchase the product, send them the link with the following text:\nHere is the product link: https://shop.samsung.com/br/galaxy-z-fold6/p\n\nQuestion: {question}\n\nAnswer: ","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt"},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"question":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"question","display_name":"question","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context","question"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":false,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-TmJY6","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":384,"height":515,"positionAbsolute":{"x":1389.9191275914766,"y":589.778611834799},"dragging":false},{"id":"ParseData-Du6ty","type":"genericNode","position":{"x":957.8488929544137,"y":1056.8484805104124},"data":{"type":"ParseData","node":{"template":{"_type":"Component","data":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"data","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"\n","name":"sep","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"template","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["data","template","sep"],"beta":false,"edited":false},"id":"ParseData-Du6ty"},"selected":false,"width":384,"height":384,"positionAbsolute":{"x":957.8488929544137,"y":1056.8484805104124},"dragging":false},{"id":"ChatInput-ZrJUy","type":"genericNode","position":{"x":109.36936366923464,"y":566.3920739995004},"data":{"type":"ChatInput","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"files","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["input_value","sender","sender_name","session_id","files"],"beta":false,"edited":false},"id":"ChatInput-ZrJUy"},"selected":false,"width":384,"height":308,"positionAbsolute":{"x":109.36936366923464,"y":566.3920739995004},"dragging":false},{"id":"SupabaseVectorStore-JmvKe","type":"genericNode","position":{"x":544.24043071561,"y":724.1947579039884},"data":{"type":"SupabaseVectorStore","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"embedding","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other"},"ingest_data":{"trace_as_input":true,"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"ingest_data","display_name":"Ingest Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List\n\nfrom langchain_community.vectorstores import SupabaseVectorStore\nfrom supabase.client import Client, create_client\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput\nfrom langflow.schema import Data\n\n\nclass SupabaseVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"Supabase\"\n    description = \"Supabase Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/vectorstores/supabase/\"\n    name = \"SupabaseVectorStore\"\n    icon = \"Supabase\"\n\n    inputs = [\n        StrInput(name=\"supabase_url\", display_name=\"Supabase URL\", required=True),\n        SecretStrInput(name=\"supabase_service_key\", display_name=\"Supabase Service Key\", required=True),\n        StrInput(name=\"table_name\", display_name=\"Table Name\", advanced=True),\n        StrInput(name=\"query_name\", display_name=\"Query Name\"),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n    ]\n\n    def build_vector_store(self) -> SupabaseVectorStore:\n        return self._build_supabase()\n\n    def _build_supabase(self) -> SupabaseVectorStore:\n        supabase: Client = create_client(self.supabase_url, supabase_key=self.supabase_service_key)\n\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        if documents:\n            supabase_vs = SupabaseVectorStore.from_documents(\n                documents=documents,\n                embedding=self.embedding,\n                query_name=self.query_name,\n                client=supabase,\n                table_name=self.table_name,\n            )\n        else:\n            supabase_vs = SupabaseVectorStore(\n                client=supabase,\n                embedding=self.embedding,\n                table_name=self.table_name,\n                query_name=self.query_name,\n            )\n\n        return supabase_vs\n\n    def search_documents(self) -> List[Data]:\n        vector_store = self._build_supabase()\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        else:\n            return []\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":4,"name":"number_of_results","display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int"},"query_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"match_documents","name":"query_name","display_name":"Query Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str"},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"search_query","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"supabase_service_key":{"load_from_db":false,"required":true,"placeholder":"","show":true,"value":"","name":"supabase_service_key","display_name":"Supabase Service Key","advanced":false,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str"},"supabase_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"","name":"supabase_url","display_name":"Supabase URL","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str"},"table_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"table_name","display_name":"Table Name","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str"}},"description":"Supabase Vector Store with search capabilities","icon":"Supabase","base_classes":["Data","Retriever"],"display_name":"Supabase","documentation":"https://python.langchain.com/v0.2/docs/integrations/vectorstores/supabase/","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true,"hidden":false},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["supabase_url","supabase_service_key","table_name","query_name","search_query","ingest_data","embedding","number_of_results"],"beta":false,"edited":false},"id":"SupabaseVectorStore-JmvKe"},"selected":false,"width":384,"height":717,"positionAbsolute":{"x":544.24043071561,"y":724.1947579039884},"dragging":false},{"id":"OllamaEmbeddings-Vufg5","type":"genericNode","position":{"x":977.5733878326851,"y":2025.3250125260438},"data":{"type":"OllamaEmbeddings","node":{"template":{"_type":"Component","base_url":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"base_url","display_name":"Ollama Base URL","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.embeddings import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"model":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"llama3:latest","name":"model","display_name":"Ollama Model","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Model Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float"}},"description":"Generate embeddings using Ollama models.","icon":"Ollama","base_classes":["Embeddings"],"display_name":"Ollama Embeddings","documentation":"https://python.langchain.com/docs/integrations/text_embedding/ollama","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["model","base_url","temperature"],"beta":false,"edited":false},"id":"OllamaEmbeddings-Vufg5"},"selected":false,"width":384,"height":410,"positionAbsolute":{"x":977.5733878326851,"y":2025.3250125260438},"dragging":false},{"id":"SupabaseVectorStore-0PPH9","type":"genericNode","position":{"x":1456.435539530744,"y":1460.3924953953635},"data":{"type":"SupabaseVectorStore","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"embedding","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other"},"ingest_data":{"trace_as_input":true,"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"ingest_data","display_name":"Ingest Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List\n\nfrom langchain_community.vectorstores import SupabaseVectorStore\nfrom supabase.client import Client, create_client\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput\nfrom langflow.schema import Data\n\n\nclass SupabaseVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"Supabase\"\n    description = \"Supabase Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/vectorstores/supabase/\"\n    name = \"SupabaseVectorStore\"\n    icon = \"Supabase\"\n\n    inputs = [\n        StrInput(name=\"supabase_url\", display_name=\"Supabase URL\", required=True),\n        SecretStrInput(name=\"supabase_service_key\", display_name=\"Supabase Service Key\", required=True),\n        StrInput(name=\"table_name\", display_name=\"Table Name\", advanced=True),\n        StrInput(name=\"query_name\", display_name=\"Query Name\"),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n    ]\n\n    def build_vector_store(self) -> SupabaseVectorStore:\n        return self._build_supabase()\n\n    def _build_supabase(self) -> SupabaseVectorStore:\n        supabase: Client = create_client(self.supabase_url, supabase_key=self.supabase_service_key)\n\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        if documents:\n            supabase_vs = SupabaseVectorStore.from_documents(\n                documents=documents,\n                embedding=self.embedding,\n                query_name=self.query_name,\n                client=supabase,\n                table_name=self.table_name,\n            )\n        else:\n            supabase_vs = SupabaseVectorStore(\n                client=supabase,\n                embedding=self.embedding,\n                table_name=self.table_name,\n                query_name=self.query_name,\n            )\n\n        return supabase_vs\n\n    def search_documents(self) -> List[Data]:\n        vector_store = self._build_supabase()\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        else:\n            return []\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":4,"name":"number_of_results","display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int"},"query_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"match_documents","name":"query_name","display_name":"Query Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str"},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"search_query","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"supabase_service_key":{"load_from_db":false,"required":true,"placeholder":"","show":true,"value":"","name":"supabase_service_key","display_name":"Supabase Service Key","advanced":false,"input_types":[],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str"},"supabase_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"","name":"supabase_url","display_name":"Supabase URL","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str"},"table_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"documents","name":"table_name","display_name":"Table Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str"}},"description":"Supabase Vector Store with search capabilities","icon":"Supabase","base_classes":["Data","Retriever"],"display_name":"Supabase","documentation":"https://python.langchain.com/v0.2/docs/integrations/vectorstores/supabase/","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true}],"field_order":["supabase_url","supabase_service_key","table_name","query_name","search_query","ingest_data","embedding","number_of_results"],"beta":false,"edited":false},"id":"SupabaseVectorStore-0PPH9"},"selected":false,"width":384,"height":802,"positionAbsolute":{"x":1456.435539530744,"y":1460.3924953953635},"dragging":false},{"id":"SplitText-D7k0W","type":"genericNode","position":{"x":972.861024418926,"y":1468.6872662337937},"data":{"type":"SplitText","node":{"template":{"_type":"Component","data_inputs":{"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"data_inputs","display_name":"Data Inputs","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to split.","title_case":false,"type":"other"},"chunk_overlap":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":200,"name":"chunk_overlap","display_name":"Chunk Overlap","advanced":false,"dynamic":false,"info":"Number of characters to overlap between chunks.","title_case":false,"type":"int"},"chunk_size":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1000,"name":"chunk_size","display_name":"Chunk Size","advanced":false,"dynamic":false,"info":"The maximum number of characters in each chunk.","title_case":false,"type":"int"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List\n\nfrom langchain_text_splitters import CharacterTextSplitter\n\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"The data to split.\",\n            input_types=[\"Data\"],\n            is_list=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum number of characters in each chunk.\",\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=\"The character to split on. Defaults to newline.\",\n            value=\"\\n\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs):\n        data = []\n        for doc in docs:\n            data.append(Data(text=doc.page_content, data=doc.metadata))\n        return data\n\n    def split_text(self) -> List[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = []\n        for _input in self.data_inputs:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"separator":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"\n","name":"separator","display_name":"Separator","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The character to split on. Defaults to newline.","title_case":false,"type":"str"}},"description":"Split text into chunks based on specified criteria.","icon":"scissors-line-dashed","base_classes":["Data"],"display_name":"Split Text","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"chunks","display_name":"Chunks","method":"split_text","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["data_inputs","chunk_overlap","chunk_size","separator"],"beta":false,"edited":false},"id":"SplitText-D7k0W"},"selected":false,"width":384,"height":527,"positionAbsolute":{"x":972.861024418926,"y":1468.6872662337937},"dragging":false},{"id":"URL-09imP","type":"genericNode","position":{"x":516.0032995130828,"y":1468.7217555009784},"data":{"type":"URL","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import re\n\nfrom langchain_community.document_loaders.web_base import WebBaseLoader\n\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\n\n\nclass URLComponent(Component):\n    display_name = \"URL\"\n    description = \"Fetch content from one or more URLs.\"\n    icon = \"layout-template\"\n    name = \"URL\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            info=\"Enter one or more URLs, separated by commas.\",\n            is_list=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"fetch_content\"),\n    ]\n\n    def ensure_url(self, string: str) -> str:\n        \"\"\"\n        Ensures the given string is a URL by adding 'http://' if it doesn't start with 'http://' or 'https://'.\n        Raises an error if the string is not a valid URL.\n\n        Parameters:\n            string (str): The string to be checked and possibly modified.\n\n        Returns:\n            str: The modified string that is ensured to be a URL.\n\n        Raises:\n            ValueError: If the string is not a valid URL.\n        \"\"\"\n        if not string.startswith((\"http://\", \"https://\")):\n            string = \"http://\" + string\n\n        # Basic URL validation regex\n        url_regex = re.compile(\n            r\"^(https?:\\/\\/)?\"  # optional protocol\n            r\"(www\\.)?\"  # optional www\n            r\"([a-zA-Z0-9.-]+)\"  # domain\n            r\"(\\.[a-zA-Z]{2,})?\"  # top-level domain\n            r\"(:\\d+)?\"  # optional port\n            r\"(\\/[^\\s]*)?$\",  # optional path\n            re.IGNORECASE,\n        )\n\n        if not url_regex.match(string):\n            raise ValueError(f\"Invalid URL: {string}\")\n\n        return string\n\n    def fetch_content(self) -> list[Data]:\n        urls = [self.ensure_url(url.strip()) for url in self.urls if url.strip()]\n        loader = WebBaseLoader(web_paths=urls, encoding=\"utf-8\")\n        docs = loader.load()\n        data = [Data(text=doc.page_content, **doc.metadata) for doc in docs]\n        self.status = data\n        return data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"urls":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":["https://www.oficinadanet.com.br/smartphones/55789-galaxy-z-fold-6-ficha-tecnica-precos"],"name":"urls","display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, separated by commas.","title_case":false,"type":"str"}},"description":"Fetch content from one or more URLs.","icon":"layout-template","base_classes":["Data"],"display_name":"URL","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"fetch_content","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["urls"],"beta":false,"edited":false},"id":"URL-09imP"},"selected":false,"width":384,"height":308,"positionAbsolute":{"x":516.0032995130828,"y":1468.7217555009784},"dragging":false},{"id":"OllamaEmbeddings-eo3Vz","type":"genericNode","position":{"x":-50.0987013063376,"y":1150.775576529191},"data":{"type":"OllamaEmbeddings","node":{"template":{"_type":"Component","base_url":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"base_url","display_name":"Ollama Base URL","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.embeddings import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"model":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"llama3:latest","name":"model","display_name":"Ollama Model","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Model Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float"}},"description":"Generate embeddings using Ollama models.","icon":"Ollama","base_classes":["Embeddings"],"display_name":"Ollama Embeddings","documentation":"https://python.langchain.com/docs/integrations/text_embedding/ollama","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["model","base_url","temperature"],"beta":false,"edited":false},"id":"OllamaEmbeddings-eo3Vz"},"selected":false,"width":384,"height":410,"positionAbsolute":{"x":-50.0987013063376,"y":1150.775576529191},"dragging":false}],"edges":[{"source":"OllamaModel-I64ie","sourceHandle":"{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-I64ieœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-4s3S0","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-4s3S0œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-4s3S0","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OllamaModel","id":"OllamaModel-I64ie","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OllamaModel-I64ie{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-I64ieœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-4s3S0{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-4s3S0œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"Prompt-TmJY6","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-TmJY6œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OllamaModel-I64ie","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-I64ieœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OllamaModel-I64ie","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-TmJY6","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-TmJY6{œdataTypeœ:œPromptœ,œidœ:œPrompt-TmJY6œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-I64ie{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-I64ieœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"ChatInput-ZrJUy","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-ZrJUyœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-TmJY6","targetHandle":"{œfieldNameœ:œquestionœ,œidœ:œPrompt-TmJY6œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"question","id":"Prompt-TmJY6","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-ZrJUy","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-ZrJUy{œdataTypeœ:œChatInputœ,œidœ:œChatInput-ZrJUyœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-TmJY6{œfieldNameœ:œquestionœ,œidœ:œPrompt-TmJY6œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""},{"source":"SplitText-D7k0W","sourceHandle":"{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-D7k0Wœ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}","target":"SupabaseVectorStore-0PPH9","targetHandle":"{œfieldNameœ:œingest_dataœ,œidœ:œSupabaseVectorStore-0PPH9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"ingest_data","id":"SupabaseVectorStore-0PPH9","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"SplitText","id":"SplitText-D7k0W","name":"chunks","output_types":["Data"]}},"id":"reactflow__edge-SplitText-D7k0W{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-D7k0Wœ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}-SupabaseVectorStore-0PPH9{œfieldNameœ:œingest_dataœ,œidœ:œSupabaseVectorStore-0PPH9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":""},{"source":"URL-09imP","sourceHandle":"{œdataTypeœ:œURLœ,œidœ:œURL-09imPœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}","target":"SplitText-D7k0W","targetHandle":"{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-D7k0Wœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data_inputs","id":"SplitText-D7k0W","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"URL","id":"URL-09imP","name":"data","output_types":["Data"]}},"id":"reactflow__edge-URL-09imP{œdataTypeœ:œURLœ,œidœ:œURL-09imPœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-SplitText-D7k0W{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-D7k0Wœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":""},{"source":"OllamaEmbeddings-Vufg5","sourceHandle":"{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-Vufg5œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}","target":"SupabaseVectorStore-0PPH9","targetHandle":"{œfieldNameœ:œembeddingœ,œidœ:œSupabaseVectorStore-0PPH9œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"embedding","id":"SupabaseVectorStore-0PPH9","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OllamaEmbeddings","id":"OllamaEmbeddings-Vufg5","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OllamaEmbeddings-Vufg5{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-Vufg5œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-SupabaseVectorStore-0PPH9{œfieldNameœ:œembeddingœ,œidœ:œSupabaseVectorStore-0PPH9œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","className":""},{"source":"ChatInput-ZrJUy","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-ZrJUyœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"SupabaseVectorStore-JmvKe","targetHandle":"{œfieldNameœ:œsearch_queryœ,œidœ:œSupabaseVectorStore-JmvKeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"search_query","id":"SupabaseVectorStore-JmvKe","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-ZrJUy","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-ZrJUy{œdataTypeœ:œChatInputœ,œidœ:œChatInput-ZrJUyœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-SupabaseVectorStore-JmvKe{œfieldNameœ:œsearch_queryœ,œidœ:œSupabaseVectorStore-JmvKeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"SupabaseVectorStore-JmvKe","sourceHandle":"{œdataTypeœ:œSupabaseVectorStoreœ,œidœ:œSupabaseVectorStore-JmvKeœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}","target":"ParseData-Du6ty","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œParseData-Du6tyœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-Du6ty","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"SupabaseVectorStore","id":"SupabaseVectorStore-JmvKe","name":"search_results","output_types":["Data"]}},"id":"reactflow__edge-SupabaseVectorStore-JmvKe{œdataTypeœ:œSupabaseVectorStoreœ,œidœ:œSupabaseVectorStore-JmvKeœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-ParseData-Du6ty{œfieldNameœ:œdataœ,œidœ:œParseData-Du6tyœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":""},{"source":"ParseData-Du6ty","sourceHandle":"{œdataTypeœ:œParseDataœ,œidœ:œParseData-Du6tyœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-TmJY6","targetHandle":"{œfieldNameœ:œcontextœ,œidœ:œPrompt-TmJY6œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context","id":"Prompt-TmJY6","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ParseData","id":"ParseData-Du6ty","name":"text","output_types":["Message"]}},"id":"reactflow__edge-ParseData-Du6ty{œdataTypeœ:œParseDataœ,œidœ:œParseData-Du6tyœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-TmJY6{œfieldNameœ:œcontextœ,œidœ:œPrompt-TmJY6œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""},{"source":"OllamaEmbeddings-eo3Vz","sourceHandle":"{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-eo3Vzœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}","target":"SupabaseVectorStore-JmvKe","targetHandle":"{œfieldNameœ:œembeddingœ,œidœ:œSupabaseVectorStore-JmvKeœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"embedding","id":"SupabaseVectorStore-JmvKe","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OllamaEmbeddings","id":"OllamaEmbeddings-eo3Vz","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OllamaEmbeddings-eo3Vz{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-eo3Vzœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-SupabaseVectorStore-JmvKe{œfieldNameœ:œembeddingœ,œidœ:œSupabaseVectorStore-JmvKeœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}","className":""}],"viewport":{"x":382.17418720469766,"y":-626.7444214420916,"zoom":0.8864026226042385}},"description":"Base Para Agente Atendimento ao Cliente","name":"Atendimento Ao Cliente Fabricando Sua Ideia","last_tested_version":"1.0.8","endpoint_name":null,"is_component":false}